# StorybenchLLM

A modular tool for evaluating the creativity of proprietary and open-source LLMs across various creative writing tasks including storytelling, screenwriting, advertising concepts, and cross-genre narratives.

## Requirements

- **Python 3.12+** (Required and fully tested)
- Node.js 18+ (for web frontend)

## Features

- **Modular Architecture**: Easy to add new model types and evaluators
- **API & Local Support**: Works with API-based models (OpenAI, Anthropic, Google) and local GGUF models
- **Web UI**: Complete web interface for configuration, evaluation, and results ‚úÖ
- **CLI Interface**: Full command-line interface for automated workflows ‚úÖ
- **Real-time Monitoring**: Live progress tracking with Server-Sent Events
- **Resume Functionality**: Smart resume from interruption points with progress detection
- **Background Processing**: Non-blocking evaluation execution
- **Results Dashboard**: Interactive results viewing with actual evaluation data
- **Robust Resume**: Can recover from crashes and continue evaluations
- **Progress Tracking**: Real-time progress with incremental saves
- **Configuration Versioning**: Handles config changes gracefully
- **Automated Evaluation**: Uses LLMs to score creativity metrics
- **Enhanced API Error Handling**: Robust retry mechanism with exponential backoff
- **Prompt Validation**: Validates structure and content of `prompts.json` during configuration loading
- **Comprehensive Testing**: 91% test success rate with Python 3.12+ compatibility validated

## Quick Start

### Web UI (Recommended)
1. **Setup Environment**
   ```bash
   python3.12 -m venv venv-storybench
   source venv-storybench/bin/activate
   pip install -e .
   ```

2. **Start Web Server**
   ```bash
   storybench-web
   ```

3. **Access Web UI**: Open http://localhost:8000
   - üìä **Results**: View evaluation results and progress
   - ‚öôÔ∏è **Configuration**: Manage models and prompts  
   - üöÄ **Evaluation**: Run evaluations with real-time monitoring
   - üìö **API Docs**: Available at http://localhost:8000/api/docs

### CLI Interface
1. **Setup Environment**
   ```bash
   python3.12 -m venv venv-storybench
   source venv-storybench/bin/activate  # On Windows: venv-storybench\Scripts\activate
   pip install -e .
   ```

2. **Configure API Keys**
   ```bash
   cp .env.example .env
   # Edit .env with your API keys
   ```

3. **Run Evaluation**
   ```bash
   # Validate configuration first
   storybench validate
   
   # Run evaluation (dry-run for testing)
   storybench evaluate --dry-run
   
   # Run actual evaluation
   storybench evaluate
   
   # Clean up models when needed
   storybench clean --models
   ```

## Configuration

- `config/models.yaml` - Model configurations and settings
- `config/prompts.json` - Creative writing prompts
- `config/evaluation_criteria.yaml` - Scoring criteria
- `.env` - API keys (not committed to git)

## Results

Results are saved in the `output/` directory as JSON files, one per model with automatic progress tracking and resume capability.

## Development & Testing

```bash
# Install development dependencies
pip install -e ".[dev]"

# Run test suite
pytest                                    # All tests
pytest -k "not selenium"                 # Skip browser tests
pytest tests/test_basic.py -v            # Basic functionality tests
pytest tests/test_comprehensive_backend.py -v  # API tests
pytest tests/test_python312_compatibility.py -v # Python 3.12+ tests

# Test coverage: 91% success rate
# - 59 tests passing
# - 3 tests skipped (intentional)
# - 1 expected failure (xfailed)
# - Python 3.12+ compatibility fully validated
```

## Completed Features

- ‚úÖ **Web UI**: Complete web interface with real-time monitoring
- ‚úÖ **CLI Interface**: Full command-line functionality
- ‚úÖ **Auto-evaluation**: System now automatically evaluates response quality using LLMs
- ‚úÖ **Local LLM Support**: Local GGUF model capability working and tested
- ‚úÖ **Python 3.12+ Support**: Fully compatible and tested with Python 3.12+
- ‚úÖ **Comprehensive Testing**: Refactored test suite with high success rate
- ‚úÖ **API Documentation**: Interactive API docs available

## Roadmap & TODOs

### High Priority
- [ ] **Database Integration**: Replace file-based storage with MongoDB Atlas
  - Migrate configuration storage from YAML/JSON files to MongoDB collections
  - Store evaluation results in database with proper indexing and querying
  - Add database connection management and error handling
  - Implement data migration tools for existing file-based data
  - Add database backup and restore functionality

- [ ] **Enhanced Web UI**: Complete the web interface with missing functionality
  - **API Key Management**: Secure interface for managing API keys (OpenAI, Anthropic, Google)
  - **Advanced Model Configuration**: UI for all model parameters (temperature, max_tokens, etc.)
  - **Prompt Editor**: Rich text editor for creating and editing creative writing prompts
  - **Evaluation Criteria Management**: Interface for customizing scoring criteria
  - **User Authentication**: Login system and user-specific configurations
  - **Results Export**: Download results as CSV, Excel, or PDF reports
  - **Evaluation History**: Timeline view of past evaluations with comparison tools

### Medium Priority
- [ ] **Performance Optimization**
  - Implement caching layer for frequent database queries
  - Add pagination for large result sets
  - Optimize frontend bundle size and loading times
  - Background job queue for long-running evaluations

- [ ] **Advanced Features**
  - **Evaluation Templates**: Pre-configured evaluation setups for different use cases
  - **Batch Evaluations**: Run multiple model comparisons simultaneously
  - **A/B Testing**: Compare different prompt variations across models
  - **Analytics Dashboard**: Statistical analysis and trend visualization
  - **API Rate Limiting**: Smart throttling for API-based models
  - **Model Performance Benchmarks**: Standardized creativity scoring across models

### Low Priority
- [ ] **Deployment & Infrastructure**
  - Docker containerization improvements
  - Kubernetes deployment manifests
  - CI/CD pipeline for automated testing and deployment
  - Production monitoring and logging
  - Automated backups and disaster recovery

- [ ] **Integrations**
  - Webhook support for evaluation completion notifications
  - Slack/Discord bot integration for team notifications
  - Integration with popular ML platforms (Weights & Biases, MLflow)
  - REST API versioning and backward compatibility

## Project Structure

```
storybench/
‚îú‚îÄ‚îÄ src/storybench/          # Main package
‚îÇ   ‚îú‚îÄ‚îÄ cli.py              # Command-line interface
‚îÇ   ‚îú‚îÄ‚îÄ evaluators/         # LLM evaluators (API & local)
‚îÇ   ‚îú‚îÄ‚îÄ models/             # Configuration models
‚îÇ   ‚îî‚îÄ‚îÄ web/                # Web interface (FastAPI)
‚îú‚îÄ‚îÄ config/                  # Configuration files  
‚îú‚îÄ‚îÄ output/                  # Evaluation outputs (gitignored)
‚îú‚îÄ‚îÄ tests/                   # Comprehensive test suite
‚îú‚îÄ‚îÄ frontend/               # Vue.js web frontend
‚îî‚îÄ‚îÄ models/                 # Downloaded local models (gitignored)
```

## Python 3.12+ Compatibility

This project requires Python 3.12+ and has been fully tested for compatibility:

- ‚úÖ **Type annotations**: Modern typing with `Optional`, `List`, `Dict`
- ‚úÖ **Async/await**: Full asyncio compatibility
- ‚úÖ **Pathlib**: Modern path handling
- ‚úÖ **FastAPI**: Web framework compatibility
- ‚úÖ **Dependencies**: All major dependencies support Python 3.12+
- ‚úÖ **Testing**: Comprehensive test suite validates 3.12+ functionality
