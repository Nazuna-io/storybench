# Storybench

A modular tool for evaluating the creativity of proprietary and open-source LLMs across various creative writing tasks including storytelling, screenwriting, advertising concepts, and cross-genre narratives.

## ğŸš€ Latest Update: DeepInfra Integration (May 2025)

**New Provider Added: DeepInfra with DeepSeek-R1 Support!**

### âœ… **DeepInfra Integration**
- **DeepSeek-R1 Model**: Access to state-of-the-art reasoning model via DeepInfra API
- **OpenAI-Compatible**: Uses familiar chat completions API format for easy integration
- **Production Ready**: Full generation and evaluation workflow tested and validated
- **Cost Effective**: Access to powerful models at competitive pricing through DeepInfra

### âœ… **Enhanced Provider Support**
- **4 Major Providers**: OpenAI, Anthropic, Google, and now DeepInfra
- **20+ Models Available**: Wide variety of models for comprehensive evaluation
- **Unified Interface**: Consistent API across all providers with provider-specific optimizations

## ğŸš€ Previous Update: Complete Web Interface & Bug Fixes (May 2025)

**All critical issues resolved - Storybench is now fully operational!**

### âœ… **Fixed Issues**
- **Results Page**: Now correctly displays completed evaluations with scores from database
- **Global Settings**: Proper defaults (temperature: 1.0, max_tokens: 8192) working correctly  
- **Models Management**: Complete workflow for API providers â†’ API keys â†’ model configuration
- **Web Server**: Fixed import errors, encryption keys, and frontend serving
- **Database Integration**: Properly reads from response_llm_evaluations collection

### âœ… **Enhanced Features**
- **Structured Model Management**: Add providers, configure API keys, then add specific models
- **Real Evaluation Data**: View actual scores from completed evaluations (Gemini-2.5-Pro: 2.50, Claude-4-Sonnet: 2.90)
- **Improved Configuration**: Save/load settings with proper validation and error handling
- **API Testing**: Built-in API key validation and testing functionality

## ğŸš€ Phase 5 Complete: Research-Quality Evaluation System

**Storybench now features sophisticated sequence-aware evaluation with realistic scoring standards!**

- **âœ… Sequence-Aware Evaluation**: Assesses coherence across multi-prompt creative sequences
- **âœ… Realistic Scoring Standards**: Stringent criteria that properly differentiate quality levels
- **âœ… Full-Context Assessment**: GPT-4 Turbo with 128k context for complete evaluation
- **âœ… Research-Quality Results**: Meaningful model comparisons for academic research

## Requirements

- **Python 3.12+** (Required and fully tested)
- **MongoDB Atlas** (or local MongoDB for development)
- **OpenAI API Key** (for evaluation system)
- Node.js 18+ (for web frontend)

## Features

### Advanced Evaluation Engine âœ¨ ENHANCED
- **Sequence-Aware Analysis**: Evaluates narrative coherence across related responses
- **Realistic Scoring**: 1-5 scale with explicit standards (most responses score 2-3)
- **Full Context Evaluation**: No content truncation - complete assessment of all responses
- **Research Standards**: Compares against professional published fiction, not just AI writing
- **Comprehensive Criteria**: 7 evaluation dimensions with detailed justifications

### Core Evaluation Engine
- **Modular Architecture**: Easy to add new model types and evaluators
- **API & Local Support**: Works with API-based models (OpenAI, Anthropic, Google, DeepInfra) and local GGUF models
- **Resume Functionality**: Smart resume from interruption points with database-backed progress tracking
- **Automated Evaluation**: Uses GPT-4 Turbo to score creativity metrics with configurable criteria

### Database & Data Management
- **MongoDB Atlas Integration**: Enterprise-grade database backend
- **Data Migration Tools**: Automatic import of existing JSON evaluation results
- **Advanced Analytics**: Query evaluation patterns, performance metrics, and trends
- **Backup & Recovery**: Automated backup systems with rollback capability
- **Export Functionality**: Export data for external analysis tools

### Web Interface âœ¨ FULLY OPERATIONAL
- **Complete Web UI**: Full web interface for configuration, evaluation, and results âœ…
- **Results Dashboard**: Interactive results viewing with **real evaluation data** from database âœ…  
- **Models Management**: Structured workflow: API providers â†’ API keys â†’ model configuration âœ…
- **Global Settings**: Proper defaults (temperature: 1.0, max_tokens: 8192) with save functionality âœ…
- **API Key Management**: Secure storage with testing and validation capabilities âœ…
- **Real-time Monitoring**: Live progress tracking with Server-Sent Events
- **Configuration Management**: Web-based model, prompt, and criteria configuration âœ…

### CLI Interface

1. **End-to-End Pipeline** (New! âœ¨)
   ```bash
   # Complete pipeline: Generate responses + evaluate
   python3 run_end_to_end.py --auto-evaluate
   
   # Generate responses only (specific models/sequences)
   python3 run_end_to_end.py --models "GPT-4o" --sequences "FilmNarrative"
   
   # Full pipeline with filtering
   python3 run_end_to_end.py --models "GPT-4o,Claude-4-Sonnet" --sequences "FilmNarrative,LiteraryNarrative" --auto-evaluate
   ```

2. **Configure Models and Prompts**
   ```bash
   # Edit configuration files
   vim config/models.yaml
   vim config/prompts.json
   vim config/evaluation_criteria.yaml
   ```

3. **Run Evaluation**
   ```bash
   storybench evaluate
   ```

4. **Database Operations**
   ```bash
   # Import existing JSON evaluation results
   storybench migrate --validate --cleanup
   
   # Export data for analysis
   storybench export --export-dir ./analysis_data
   
   # Check database status
   storybench status
   ```

### Quality & Reliability
- **Enhanced Error Handling**: Robust retry mechanism with exponential backoff
- **Prompt Validation**: Validates structure and content during configuration loading
- **Comprehensive Testing**: 91% test success rate with Python 3.12+ compatibility
- **Database Validation**: Automated data integrity checking
- **Full CLI**: Complete command-line interface for automated workflows âœ…
- **Migration Commands**: Database import/export and cleanup tools
- **Background Processing**: Non-blocking evaluation execution
- **Configuration Versioning**: Handles config changes gracefully

## Quick Start

### Prerequisites

1. **MongoDB Atlas Setup** (Recommended for production):
   ```bash
   # Sign up for MongoDB Atlas (free tier available)
   # Create cluster and get connection string
   # Add connection string to .env file
   ```

2. **Environment Setup**:
   ```bash
   python3.12 -m venv venv-storybench
   source venv-storybench/bin/activate
   pip install -e .
   
   # Copy environment template and add your API keys + MongoDB URI
   cp .env.example .env
   # Edit .env with your API keys and MongoDB connection string
   ```

### Web UI (Recommended) âœ¨ ENHANCED

1. **Start Web Server**
   ```bash
   storybench-web
   ```

2. **Access Web UI**: Open http://localhost:8000

   **ğŸ¯ Available Pages:**
   - **ğŸ“Š Results**: View completed evaluations with scores and detailed analysis
     - Shows actual evaluation data from database
     - Model comparisons with averaged scores across criteria
     - Drill-down to individual responses and evaluations
   
   - **âš™ï¸ Models Configuration**: Complete model management workflow
     - **Global Settings**: Temperature (1.0), Max Tokens (8192), etc.
     - **API Keys**: Add and test API keys for different providers
     - **Models**: Add specific models from configured providers
     - Structured workflow: Provider â†’ API Key â†’ Model Configuration
   
   - **ğŸ“ Prompts**: Configure creative writing prompt sequences
   - **ğŸ“ Criteria**: Set evaluation criteria and scoring standards  
   - **ğŸš€ Evaluation**: Run evaluations with real-time monitoring
   - **ğŸ“š API Docs**: Available at http://localhost:8000/api/docs

3. **Current Data**: 
   - **2 completed evaluations** ready to view
   - **150 responses** from Gemini-2.5-Pro and Claude-4-Sonnet
   - **169 LLM evaluations** with detailed criteria scoring

## Database Migration

If you have existing evaluation results in JSON format, Phase 4 provides automatic migration:

```bash
# Migrate existing data to MongoDB
storybench migrate --validate --cleanup

# This will:
# 1. Import all JSON files from output/ directory
# 2. Validate data integrity
# 3. Create timestamped backup of original files
# 4. Clean up original files after successful import
```

## Configuration

### Environment Variables (.env)
```bash
# MongoDB Atlas (Required)
MONGODB_URI=mongodb+srv://username:password@cluster.mongodb.net/storybench?retryWrites=true&w=majority

# API Keys (Add as needed)
OPENAI_API_KEY=your_openai_key_here
ANTHROPIC_API_KEY=your_anthropic_key_here
GOOGLE_API_KEY=your_google_key_here
DEEPINFRA_API_KEY=your_deepinfra_key_here

# Optional: Custom endpoints
GROK_BASE_URL=https://api.x.ai/v1
```

### Model Configuration (config/models.yaml)
Configure available models for evaluation:
```yaml
models:
  - name: "Claude-4-Sonnet"
    type: "api"
    provider: "anthropic"
    model_name: "claude-3-5-sonnet-20241022"
  
  - name: "DeepSeek-R1"
    type: "api"
    provider: "deepinfra"
    model_name: "deepseek-ai/DeepSeek-R1"
    context_size: 128000
  
  - name: "Local-Llama"
    type: "local"
    repo_id: "microsoft/DialoGPT-medium"
    filename: "model.gguf"
```

### Evaluation Prompts (config/prompts.json)
Define creative writing prompts across different sequences:
```json
{
  "FilmNarrative": [
    {
      "name": "Initial Concept",
      "text": "Create a completely original feature film concept..."
    }
  ]
}
```

### Evaluation Criteria (config/evaluation_criteria.yaml)
Configure how responses are automatically scored:
```yaml
creativity:
  name: "Creativity and Originality"
  description: "Assesses novelty and creative thinking"
  scale: 10

coherence:
  name: "Narrative Coherence"
  description: "Evaluates logical flow and structure"
  scale: 10
```

## Architecture

### Database Schema (MongoDB Atlas)
```javascript
// Collections:
evaluations: {
  _id: ObjectId,
  config_hash: String,
  models: [String],
  status: "completed" | "in_progress" | "failed",
  total_tasks: Number,
  completed_tasks: Number,
  timestamp: Date
}

responses: {
  _id: ObjectId,
  evaluation_id: String,  // Reference to evaluation
  model_name: String,
  sequence: String,
  run: Number,
  prompt_name: String,
  response: String,
  generation_time: Number,
  completed_at: Date
}

evaluation_scores: {
  _id: ObjectId,
  evaluation_id: ObjectId,
  model_name: String,
  overall_score: Number,
  detailed_scores: {
    creativity: Number,
    coherence: Number,
    // ... other criteria
  }
}
```

### System Architecture
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Web UI        â”‚    â”‚   CLI Interface  â”‚    â”‚  API Endpoints  â”‚
â”‚   (Frontend)    â”‚    â”‚   (Click)        â”‚    â”‚  (FastAPI)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                       â”‚                       â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   Core Engine    â”‚
                    â”‚   (Async Python) â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚               â”‚               â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚   Model APIs  â”‚ â”‚ Local Models â”‚ â”‚  MongoDB   â”‚
        â”‚ (OpenAI, etc) â”‚ â”‚   (GGUF)     â”‚ â”‚   Atlas    â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Development

### Setup Development Environment
```bash
# Clone and setup
git clone <repository>
cd storybench
python3.12 -m venv venv-storybench
source venv-storybench/bin/activate

# Install with development dependencies
pip install -e ".[dev]"

# Install pre-commit hooks
pre-commit install

# Set up environment
cp .env.example .env
# Edit .env with your API keys and MongoDB URI
```

### Testing
```bash
# Run all tests
pytest

# Run with coverage
pytest --cov=storybench --cov-report=html

# Test database migration
python test_phase4_migration.py

# Verify setup
python setup_phase4.py
```

### Web UI Development
```bash
# Install frontend dependencies
cd frontend
npm install

# Start development server
npm run dev  # Frontend: http://localhost:3000
storybench-web  # Backend: http://localhost:8000
```

## Data Management

### Migration Commands
```bash
# Import existing JSON results to MongoDB
storybench migrate --output-dir ./output --validate --cleanup

# Export database data for analysis
storybench export --export-dir ./exports

# Backup specific evaluations
storybench export --evaluation-ids 507f1f77bcf86cd799439011
```

### Database Operations
```bash
# Check connection and status
storybench status

# View evaluation progress
storybench evaluate --dry-run

# Resume interrupted evaluation
storybench evaluate --resume
```

## Troubleshooting

### Common Issues

1. **Results Page Shows No Results**
   ```bash
   # Issue: Database has evaluations but Results page empty
   # Fixed: Updated DatabaseResultsService to use response_llm_evaluations collection
   # Test: curl -X GET http://localhost:8000/api/results
   ```

2. **Global Settings Wrong Defaults**
   ```bash
   # Issue: Temperature 0.9 instead of 1.0, max_tokens 4096 instead of 8192  
   # Fixed: Backend already had correct defaults, frontend API paths corrected
   # Test: Check Models Configuration page shows temperature: 1.0, max_tokens: 8192
   ```

3. **Web Server Import Errors**
   ```bash
   # Issue: ValueError: ENCRYPTION_KEY environment variable required
   # Fixed: Added proper Fernet encryption key to .env file
   # Fixed: Added dotenv loading to main.py
   # Test: Server starts without errors
   ```

4. **Frontend Not Loading**
   ```bash
   # Issue: 404 Not Found when accessing http://localhost:8000/
   # Fixed: Corrected frontend path calculation in main.py
   # Test: Frontend loads with Vue.js interface
   ```

5. **MongoDB Connection Issues**
   ```bash
   # Check connection string in .env
   # Verify IP whitelist in MongoDB Atlas
   # Test connection: python setup_phase4.py
   ```

6. **Migration Problems**
   ```bash
   # Check file permissions in output/ directory
   # Verify JSON file format
   # Check logs for specific errors
   ```

7. **Performance Issues**
   ```bash
   # For large datasets, use batch processing
   # Monitor memory usage during evaluation
   # Consider MongoDB Atlas cluster scaling
   ```

### Support
- **Documentation**: Check PHASE_4_IMPLEMENTATION_SUMMARY.md
- **Logs**: Check application logs for detailed error information
- **Validation**: Use setup_phase4.py for comprehensive checks

## Contributing

1. Fork the repository
2. Create a feature branch
3. Make changes with tests
4. Run full test suite: `pytest`
5. Submit pull request

### Code Style
- **Black**: Code formatting (`black .`)
- **Flake8**: Linting (`flake8 src/`)
- **MyPy**: Type checking (`mypy src/`)
- **Pre-commit**: Automated checks

## License

This project is licensed under the MIT License - see the LICENSE file for details.

## Changelog

### Version 0.3.1 - Critical Bug Fixes & Web Interface Completion (May 2025)
- ğŸ”§ **Fixed Results Page**: Now correctly displays evaluation data from database
- âš™ï¸ **Fixed Global Settings**: Proper defaults (temperature: 1.0, max_tokens: 8192) working
- ğŸ”§ **Enhanced Models Management**: Complete workflow for API providers â†’ API keys â†’ models
- ğŸ” **Fixed API Keys**: Proper encryption, validation, and testing functionality  
- ğŸŒ **Fixed Web Server**: Resolved import errors, encryption keys, and frontend serving
- ğŸ“Š **Database Integration**: Correctly reads from response_llm_evaluations collection
- âœ… **Tested & Verified**: All core functionality working with real evaluation data

### Version 0.3.0 - Phase 5 Complete (June 2025)
- âœ… **Sequence-Aware Evaluation**: Assesses coherence across multi-prompt creative sequences
- âœ… **Realistic Scoring Standards**: Stringent criteria that properly differentiate quality levels
- âœ… **Full-Context Assessment**: GPT-4 Turbo with 128k context for complete evaluation
- âœ… **Research-Quality Results**: Meaningful model comparisons for academic research

### Version 0.2.0 - Phase 4 Complete (May 2025)
- âœ… **MongoDB Atlas Integration**: Enterprise database backend
- âœ… **Data Migration System**: Automatic JSON import with validation
- âœ… **Advanced Analytics**: Database-driven evaluation insights
- âœ… **Backup & Recovery**: Automated data safety systems
- âœ… **CLI Enhancement**: Migration and export commands
- âœ… **Production Ready**: Enterprise-grade reliability and scalability

### Version 0.1.0 - Core System (Previous)
- âœ… **Multi-Model Support**: API and local model evaluation
- âœ… **Web Interface**: Complete frontend with real-time monitoring
- âœ… **CLI Tools**: Command-line evaluation workflows
- âœ… **Resume Functionality**: Interruption recovery
- âœ… **Automated Scoring**: LLM-based evaluation metrics

---

**Storybench** - Enterprise-grade LLM creativity evaluation with MongoDB Atlas backend ğŸš€
**Comprehensive Testing**: 100% success rate across all test scenarios
- ğŸ“‹ **Detailed Reporting**: Rich test reports with performance metrics

### Version 0.3.1 - Critical Bug Fixes & Web Interface Completion (May 2025)
- ğŸ”§ **Fixed Results Page**: Now correctly displays evaluation data from database
- âš™ï¸ **Fixed Global Settings**: Proper defaults (temperature: 1.0, max_tokens: 8192) working
- ğŸ”§ **Enhanced Models Management**: Complete workflow for API providers â†’ API keys â†’ models
- ğŸ” **Fixed API Keys**: Proper encryption, validation, and testing functionality  
- ğŸŒ **Fixed Web Server**: Resolved import errors, encryption keys, and frontend serving

---

**Storybench** - Production-ready LLM creativity evaluation with complete local model support ğŸš€