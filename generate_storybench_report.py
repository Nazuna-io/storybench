#!/usr/bin/env python3
"""
Storybench Automated Report Generator

This script generates comprehensive analysis reports from Storybench evaluation data.
It can be run whenever new evaluation data is available to produce updated reports.

Usage:
    python generate_storybench_report.py [data_file] [--output output_file] [--format markdown|html|json]
    
Example:
    python generate_storybench_report.py full_api_production_test_report_20250528_212154.json --output report.md --format markdown
"""

import json
import sys
import argparse
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any, Optional
from collections import defaultdict
import statistics
import re

class StorybenchReportGenerator:
    def __init__(self, data_file: str):
        self.data_file = Path(data_file)
        self.data = self._load_data()
        self.models = self._extract_models()
        self.evaluations = self._process_evaluations()
        self.model_stats = self._calculate_model_statistics()
        
    def _load_data(self) -> Dict[str, Any]:
        """Load and validate the JSON data file."""
        if not self.data_file.exists():
            raise FileNotFoundError(f"Data file not found: {self.data_file}")
            
        with open(self.data_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
            
        # Validate required fields
        required_fields = ['responses', 'evaluations']
        for field in required_fields:
            if field not in data:
                raise ValueError(f"Required field '{field}' not found in data file")
                
        print(f"‚úì Loaded {len(data['responses'])} responses and {len(data['evaluations'])} evaluations")
        return data
        
    def _extract_models(self) -> List[str]:
        """Extract unique model names from responses."""
        models = set()
        for response in self.data['responses']:
            if 'model_name' in response:
                models.add(response['model_name'])
        return sorted(list(models))
        
    def _process_evaluations(self) -> Dict[str, Dict]:
        """Process evaluations and extract scores."""
        evaluations_by_response = {}
        
        for eval_data in self.data['evaluations']:
            response_id = eval_data['response_id']
            
            # Parse evaluation text to extract scores
            eval_text = eval_data.get('evaluation_text', '')
            scores = self._parse_evaluation_scores(eval_text)
            
            evaluations_by_response[response_id] = {
                'scores': scores,
                'evaluation_text': eval_text,
                'evaluator': eval_data.get('evaluating_llm_model', 'unknown'),
                'evaluation_time': eval_data.get('evaluation_time', 0)
            }
            
        print(f"‚úì Processed {len(evaluations_by_response)} evaluations")
        return evaluations_by_response
        
    def _parse_evaluation_scores(self, eval_text: str) -> Dict[str, float]:
        """Parse numerical scores from evaluation text."""
        criteria = [
            'creativity', 'coherence', 'character_depth', 'dialogue_quality',
            'visual_imagination', 'conceptual_depth', 'adaptability'
        ]
        
        scores = {}
        
        for criterion in criteria:
            # Look for pattern like "creativity**: 5" or "creativity: 4.5"
            patterns = [
                rf'\*\*{criterion}\*\*:\s*(\d+(?:\.\d+)?)',
                rf'{criterion}:\s*(\d+(?:\.\d+)?)',
                rf'{criterion}.*?(\d+(?:\.\d+)?)\s*[-‚Äì]',
            ]
            
            for pattern in patterns:
                match = re.search(pattern, eval_text, re.IGNORECASE)
                if match:
                    scores[criterion] = float(match.group(1))
                    break
                    
        return scores
        
    def _calculate_model_statistics(self) -> Dict[str, Dict]:
        """Calculate comprehensive statistics for each model."""
        model_stats = {}
        
        for model in self.models:
            model_responses = [r for r in self.data['responses'] if r.get('model_name') == model]
            model_evaluations = []
            
            for response in model_responses:
                response_id = response.get('_id')
                if response_id in self.evaluations:
                    eval_data = self.evaluations[response_id]
                    if eval_data['scores']:  # Only include if we have scores
                        model_evaluations.append(eval_data)
            
            if model_evaluations:
                stats = self._compute_model_stats(model, model_responses, model_evaluations)
                model_stats[model] = stats
                
        return model_stats
        
    def _compute_model_stats(self, model_name: str, responses: List[Dict], evaluations: List[Dict]) -> Dict:
        """Compute detailed statistics for a single model."""
        criteria = ['creativity', 'coherence', 'character_depth', 'dialogue_quality',
                   'visual_imagination', 'conceptual_depth', 'adaptability']
        
        # Aggregate scores by criterion
        criterion_scores = {criterion: [] for criterion in criteria}
        
        for eval_data in evaluations:
            scores = eval_data['scores']
            for criterion in criteria:
                if criterion in scores:
                    criterion_scores[criterion].append(scores[criterion])
        
        # Calculate statistics
        stats = {
            'model_name': model_name,
            'total_responses': len(responses),
            'evaluated_responses': len(evaluations),
            'provider': self._extract_provider(model_name),
            'avg_generation_time': statistics.mean([r.get('generation_time', 0) for r in responses if r.get('generation_time')]) if responses else 0,
            'criterion_stats': {}
        }
        
        # Calculate per-criterion statistics
        overall_scores = []
        for criterion in criteria:
            if criterion_scores[criterion]:
                scores = criterion_scores[criterion]
                stats['criterion_stats'][criterion] = {
                    'mean': statistics.mean(scores),
                    'median': statistics.median(scores),
                    'std_dev': statistics.stdev(scores) if len(scores) > 1 else 0,
                    'min': min(scores),
                    'max': max(scores),
                    'count': len(scores)
                }
                overall_scores.extend(scores)
        
        # Calculate overall statistics
        if overall_scores:
            stats['overall_score'] = statistics.mean(overall_scores)
            stats['overall_median'] = statistics.median(overall_scores)
            stats['overall_std_dev'] = statistics.stdev(overall_scores) if len(overall_scores) > 1 else 0
        else:
            stats['overall_score'] = 0
            stats['overall_median'] = 0
            stats['overall_std_dev'] = 0
            
        return stats
        
    def _extract_provider(self, model_name: str) -> str:
        """Extract provider name from model name."""
        if 'claude' in model_name.lower():
            return 'Anthropic'
        elif 'gemini' in model_name.lower():
            return 'Google'
        elif 'gpt' in model_name.lower() or 'o4' in model_name.lower():
            return 'OpenAI'
        elif '/' in model_name:  # Format like "deepseek-ai/model"
            return 'Deepinfra'
        else:
            return 'Unknown'
            
    def generate_markdown_report(self, include_examples: bool = True) -> str:
        """Generate a comprehensive markdown report."""
        report_parts = []
        
        # Header
        report_parts.append(self._generate_header())
        
        # Executive Summary
        report_parts.append(self._generate_executive_summary())
        
        # Performance Rankings
        report_parts.append(self._generate_rankings_table())
        
        # Detailed Score Matrix
        report_parts.append(self._generate_score_matrix())
        
        # Individual Model Analysis
        if include_examples:
            report_parts.append(self._generate_model_analysis_with_examples())
        else:
            report_parts.append(self._generate_model_analysis())
            
        # Performance by Criterion
        report_parts.append(self._generate_criterion_analysis())
        
        # Provider Analysis
        report_parts.append(self._generate_provider_analysis())
        
        # Recommendations
        report_parts.append(self._generate_recommendations())
        
        # Methodology
        report_parts.append(self._generate_methodology())
        
        return '\n\n---\n\n'.join(report_parts)
        
    def _generate_header(self) -> str:
        """Generate report header with metadata."""
        test_batch = self._extract_test_batch()
        total_responses = len(self.data['responses'])
        total_evaluations = len(self.data['evaluations'])
        
        return f"""# Comprehensive Storybench Model Performance Report

**Analysis Date:** {datetime.now().strftime('%B %d, %Y')}  
**Test Batch:** {test_batch}  
**Total Responses:** {total_responses} ({total_responses // len(self.models)} per model)  
**Total Evaluations:** {total_evaluations} ({total_evaluations / total_responses * 100:.1f}% parsing success)  
**Evaluation Criteria:** 7 dimensions of creative writing performance"""

    def _extract_test_batch(self) -> str:
        """Extract test batch identifier from data."""
        if self.data['responses']:
            return self.data['responses'][0].get('test_batch', 'unknown')
        return 'unknown'
        
    def _generate_executive_summary(self) -> str:
        """Generate executive summary with key insights."""
        sorted_models = sorted(self.model_stats.items(), 
                             key=lambda x: x[1]['overall_score'], 
                             reverse=True)
        
        top_model = sorted_models[0][1]['model_name'] if sorted_models else 'N/A'
        top_score = sorted_models[0][1]['overall_score'] if sorted_models else 0
        
        # Find creativity leader
        creativity_leader = max(self.model_stats.items(),
                              key=lambda x: x[1]['criterion_stats'].get('creativity', {}).get('mean', 0))
        creativity_score = creativity_leader[1]['criterion_stats'].get('creativity', {}).get('mean', 0)
        
        return f"""## Executive Summary

This comprehensive analysis of {len(self.models)} frontier language models reveals significant insights about creative writing capabilities across different providers and model architectures. **{top_model} achieves the highest overall performance** with a score of {top_score:.2f}/5.0, demonstrating specialized optimization for creative tasks.

**Key Findings:**
- {top_model} leads overall performance despite architectural differences
- Creativity remains challenging, with top score of {creativity_score:.2f}/5.0 ({creativity_leader[0]})
- Performance gap between top and bottom models: {sorted_models[0][1]['overall_score'] - sorted_models[-1][1]['overall_score']:.2f} points
- All models show consistent patterns in strengths and weaknesses"""

    def _generate_rankings_table(self) -> str:
        """Generate overall performance rankings table."""
        sorted_models = sorted(self.model_stats.items(), 
                             key=lambda x: x[1]['overall_score'], 
                             reverse=True)
        
        table_parts = [
            "## Overall Performance Rankings",
            "",
            "| Rank | Model | Overall Score | Provider | Responses | Evaluations |",
            "|------|-------|---------------|----------|-----------|-------------|"
        ]
        
        medals = ["ü•á", "ü•à", "ü•â"]
        
        for i, (model_name, stats) in enumerate(sorted_models):
            rank = i + 1
            medal = medals[i] if i < 3 else ""
            
            row = f"| {medal} {rank} | {model_name} | **{stats['overall_score']:.2f}/5.0** | {stats['provider']} | {stats['total_responses']}/{stats['total_responses']} | {stats['evaluated_responses']}/{stats['evaluated_responses']} |"
            table_parts.append(row)
            
        return '\n'.join(table_parts)
        
    def _generate_score_matrix(self) -> str:
        """Generate detailed score matrix for all models."""
        criteria = ['creativity', 'coherence', 'character_depth', 'dialogue_quality',
                   'visual_imagination', 'conceptual_depth', 'adaptability']
        
        sorted_models = sorted(self.model_stats.items(), 
                             key=lambda x: x[1]['overall_score'], 
                             reverse=True)
        
        # Create header
        header = "| Model | Overall | " + " | ".join(c.replace('_', ' ').title() for c in criteria) + " |"
        separator = "|" + "|".join(["-------"] * (len(criteria) + 2)) + "|"
        
        table_parts = [
            "## Detailed Score Matrix",
            "",
            header,
            separator
        ]
        
        for model_name, stats in sorted_models:
            scores = []
            for criterion in criteria:
                if criterion in stats['criterion_stats']:
                    score = stats['criterion_stats'][criterion]['mean']
                    formatted_score = f"**{score:.2f}**" if score >= 4.5 else f"{score:.2f}"
                    scores.append(formatted_score)
                else:
                    scores.append("N/A")
                    
            row = f"| **{model_name}** | **{stats['overall_score']:.2f}** | " + " | ".join(scores) + " |"
            table_parts.append(row)
            
        return '\n'.join(table_parts)
        
    def _generate_model_analysis(self) -> str:
        """Generate individual model analysis without examples."""
        sorted_models = sorted(self.model_stats.items(), 
                             key=lambda x: x[1]['overall_score'], 
                             reverse=True)
        
        analysis_parts = ["## Individual Model Analysis"]
        
        for i, (model_name, stats) in enumerate(sorted_models[:6]):  # Top 6 models
            rank_emoji = ["ü•á", "ü•à", "ü•â", "", "", ""][i]
            
            # Find strengths and weaknesses
            criterion_means = {k: v['mean'] for k, v in stats['criterion_stats'].items()}
            strengths = sorted(criterion_means.items(), key=lambda x: x[1], reverse=True)[:3]
            weaknesses = sorted(criterion_means.items(), key=lambda x: x[1])[:2]
            
            analysis = f"""### {rank_emoji} {i+1}. {model_name} ({stats['overall_score']:.2f}/5.0)

**Strengths:**
{chr(10).join([f"- **{k.replace('_', ' ').title()}** ({v:.2f}/5.0)" for k, v in strengths])}

**Weaknesses:**
{chr(10).join([f"- **{k.replace('_', ' ').title()}** ({v:.2f}/5.0)" for k, v in weaknesses])}

**Best Use Cases:**
- Content requiring {strengths[0][0].replace('_', ' ')}
- {stats['provider']} ecosystem integration
- Performance: {stats['avg_generation_time']:.1f}s average generation time"""

            analysis_parts.append(analysis)
            
        return '\n\n'.join(analysis_parts)
        
    def _generate_model_analysis_with_examples(self) -> str:
        """Generate model analysis with response examples."""
        # This would require implementing example extraction from responses
        # For now, fall back to basic analysis
        return self._generate_model_analysis()
        
    def _generate_criterion_analysis(self) -> str:
        """Generate performance analysis by criterion."""
        criteria = ['creativity', 'coherence', 'character_depth', 'dialogue_quality',
                   'visual_imagination', 'conceptual_depth', 'adaptability']
        
        analysis_parts = ["## Performance by Criterion"]
        
        # Find leaders for each criterion
        leaders_table = ["| Criterion | Leader | Score | Runner-up | Score |",
                        "|-----------|--------|-------|-----------|-------|"]
        
        for criterion in criteria:
            # Find top 2 performers for this criterion
            model_scores = []
            for model_name, stats in self.model_stats.items():
                if criterion in stats['criterion_stats']:
                    score = stats['criterion_stats'][criterion]['mean']
                    model_scores.append((model_name, score))
                    
            model_scores.sort(key=lambda x: x[1], reverse=True)
            
            if len(model_scores) >= 2:
                leader = model_scores[0]
                runner_up = model_scores[1]
                
                row = f"| **{criterion.replace('_', ' ').title()}** | {leader[0]} | **{leader[1]:.2f}** | {runner_up[0]} | {runner_up[1]:.2f} |"
                leaders_table.append(row)
                
        analysis_parts.extend([
            "### üéØ Category Leaders",
            "",
            *leaders_table
        ])
        
        return '\n'.join(analysis_parts)
        
    def _generate_provider_analysis(self) -> str:
        """Generate analysis by provider."""
        # Group models by provider
        provider_stats = defaultdict(list)
        for model_name, stats in self.model_stats.items():
            provider_stats[stats['provider']].append((model_name, stats))
            
        analysis_parts = [
            "## Provider Analysis",
            "",
            "### üè¢ Performance by Provider",
            "",
            "| Provider | Models | Average Score | Best Model | Best Score |",
            "|----------|--------|---------------|------------|------------|"
        ]
        
        provider_summary = []
        for provider, models in provider_stats.items():
            avg_score = statistics.mean([stats['overall_score'] for _, stats in models])
            best_model = max(models, key=lambda x: x[1]['overall_score'])
            
            provider_summary.append((provider, len(models), avg_score, best_model[0], best_model[1]['overall_score']))
            
        # Sort by average score
        provider_summary.sort(key=lambda x: x[2], reverse=True)
        
        for provider, count, avg_score, best_model, best_score in provider_summary:
            row = f"| **{provider}** | {count} | **{avg_score:.2f}** | {best_model} | {best_score:.2f} |"
            analysis_parts.append(row)
            
        return '\n'.join(analysis_parts)
        
    def _generate_recommendations(self) -> str:
        """Generate actionable recommendations."""
        sorted_models = sorted(self.model_stats.items(), 
                             key=lambda x: x[1]['overall_score'], 
                             reverse=True)
        
        top_3 = sorted_models[:3]
        
        return f"""## Recommendations

### üéØ Model Selection Guidelines

**For Professional Creative Writing:**
- **Primary:** {top_3[0][0]}
- **Backup:** {top_3[1][0]}
- **Budget Option:** {top_3[2][0]}

**For Commercial Applications:**
- **Reliable Quality:** {top_3[0][0]}
- **Cost-Effective:** {top_3[2][0]}
- **High Volume:** Models with fastest generation time

### üöÄ Future Development Areas

Based on evaluation patterns, all models show room for improvement in:

1. **Creativity:** Maximum observed score {max([stats['criterion_stats'].get('creativity', {}).get('mean', 0) for stats in self.model_stats.values()]):.2f}/5.0
2. **Dialogue Quality:** Wide performance variation observed
3. **Sequence Coherence:** Multi-part prompt consistency
4. **Character Development:** Psychological depth and consistency"""

    def _generate_methodology(self) -> str:
        """Generate methodology and validation section."""
        total_responses = len(self.data['responses'])
        total_evaluations = len(self.data['evaluations'])
        
        return f"""## Methodology and Validation

**Test Structure:**
- {len(self.models)} models √ó multiple prompts √ó multiple runs = {total_responses} responses
- Evaluator: {self.data['evaluations'][0].get('evaluating_llm_model', 'Unknown') if self.data['evaluations'] else 'Unknown'}
- {total_evaluations / total_responses * 100:.1f}% evaluation parsing success rate

**Evaluation Criteria (7 dimensions):**
1. Creativity (originality, innovation)
2. Coherence (logical consistency, flow)
3. Character Depth (psychological complexity)
4. Dialogue Quality (naturalness, engagement)
5. Visual Imagination (descriptive power, imagery)
6. Conceptual Depth (thematic sophistication)  
7. Adaptability (prompt following, format flexibility)

**Validation:** This analysis is based on rigorous evaluation of {total_responses} responses with comprehensive qualitative assessment of each dimension, providing high confidence in the results and rankings presented."""

    def generate_html_report(self) -> str:
        """Generate HTML version of the report."""
        markdown_content = self.generate_markdown_report()
        # Would implement markdown to HTML conversion here
        # For now, return wrapped markdown
        return f"""<!DOCTYPE html>
<html>
<head>
    <title>Storybench Analysis Report</title>
    <style>
        body {{ font-family: Arial, sans-serif; max-width: 1200px; margin: 0 auto; padding: 20px; }}
        table {{ border-collapse: collapse; width: 100%; }}
        th, td {{ border: 1px solid #ddd; padding: 8px; text-align: left; }}
        th {{ background-color: #f2f2f2; }}
    </style>
</head>
<body>
    <pre>{markdown_content}</pre>
</body>
</html>"""

    def generate_json_report(self) -> str:
        """Generate JSON version of the report data."""
        return json.dumps({
            'metadata': {
                'analysis_date': datetime.now().isoformat(),
                'test_batch': self._extract_test_batch(),
                'total_responses': len(self.data['responses']),
                'total_evaluations': len(self.data['evaluations']),
                'models_analyzed': len(self.models)
            },
            'model_statistics': self.model_stats,
            'rankings': sorted(self.model_stats.items(), 
                             key=lambda x: x[1]['overall_score'], 
                             reverse=True)
        }, indent=2)

def main():
    parser = argparse.ArgumentParser(description='Generate Storybench analysis reports')
    parser.add_argument('data_file', help='Path to JSON data file')
    parser.add_argument('--output', '-o', help='Output file path')
    parser.add_argument('--format', '-f', choices=['markdown', 'html', 'json'], 
                       default='markdown', help='Output format')
    parser.add_argument('--examples', action='store_true', 
                       help='Include response examples in report')
    
    args = parser.parse_args()
    
    try:
        # Initialize generator
        print(f"Loading data from {args.data_file}...")
        generator = StorybenchReportGenerator(args.data_file)
        
        # Generate report
        print(f"Generating {args.format} report...")
        if args.format == 'markdown':
            content = generator.generate_markdown_report(include_examples=args.examples)
        elif args.format == 'html':
            content = generator.generate_html_report()
        elif args.format == 'json':
            content = generator.generate_json_report()
        
        # Output handling
        if args.output:
            output_path = Path(args.output)
            with open(output_path, 'w', encoding='utf-8') as f:
                f.write(content)
            print(f"‚úì Report saved to {output_path}")
        else:
            print(content)
            
    except Exception as e:
        print(f"‚ùå Error: {e}", file=sys.stderr)
        sys.exit(1)

if __name__ == '__main__':
    main()
