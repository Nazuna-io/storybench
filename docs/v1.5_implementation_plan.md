# StoryBench v1.5 Implementation Plan

## Overview
StoryBench v1.5 will modernize the evaluation pipeline with automated model management, LiteLLM integration, and a new Streamlit dashboard, while preserving the core evaluation methodology.

## Project Goals
1. **Simplify model management** with YAML configuration
2. **Automate pipeline execution** with resume/skip capabilities  
3. **Integrate LiteLLM** for unified API access (keeping LangChain for context)
4. **Create Streamlit dashboard** for visualization and monitoring
5. **Remove orphaned code** (Vue.js frontend)
6. **Maintain backwards compatibility** with existing data

## Timeline Estimate
- **Total Duration**: 3-4 weeks
- **Phase 1**: 3-4 days (Foundation)
- **Phase 2**: 4-5 days (LiteLLM Integration)
- **Phase 3**: 3-4 days (Pipeline Automation)
- **Phase 4**: 5-7 days (Streamlit Dashboard)
- **Phase 5**: 2-3 days (Cleanup & Testing)
- **Phase 6**: 2-3 days (Documentation & Deployment)

---

## Phase 1: Foundation & Configuration System (Days 1-4)

### Objectives
- Set up v1.5 branch and development environment
- Create configuration system for model management
- Establish testing framework

### Deliverables

#### 1.1 Branch Setup
- [ ] Create v1.5 branch from main
- [ ] Update version in setup.py and relevant files
- [ ] Create v1.5 changelog file

#### 1.2 Configuration System
- [ ] Create `config/` directory structure
- [ ] Implement `config/models.yaml` with all current models
- [ ] Create `config/models.example.yaml` template
- [ ] Implement configuration loader class
- [ ] Add configuration validation

#### 1.3 Testing Framework
- [ ] Create `tests/test_config_loader.py`
- [ ] Add configuration validation tests
- [ ] Set up test data fixtures
- [ ] Create integration test structure

### Success Criteria
- Configuration system loads and validates model definitions
- All 12 current models defined in YAML
- Tests pass for configuration loading

---

## Phase 2: LiteLLM Integration (Days 5-9)

### Objectives
- Integrate LiteLLM while preserving LangChain context management
- Create unified model interface
- Maintain backwards compatibility

### Deliverables

#### 2.1 LiteLLM Setup
- [ ] Add litellm to requirements.txt
- [ ] Create `src/storybench/evaluators/litellm_evaluator.py`
- [ ] Implement LiteLLMEvaluator class
- [ ] Add provider-specific configurations

#### 2.2 Context Management Integration
- [ ] Ensure LangChain context manager works with LiteLLM
- [ ] Test context limits with each provider
- [ ] Verify no truncation occurs
- [ ] Add context overflow tests

#### 2.3 Provider Testing
- [ ] Test Anthropic models via LiteLLM
- [ ] Test OpenAI models (including o3/o4 special handling)
- [ ] Test Google/Gemini models
- [ ] Test DeepInfra models
- [ ] Create provider-specific test suite

#### 2.4 Backwards Compatibility
- [ ] Create adapter for existing APIEvaluator calls
- [ ] Ensure response format matches current system
- [ ] Test with existing evaluation pipeline
- [ ] Verify MongoDB storage compatibility

### Success Criteria
- All models work through LiteLLM
- Context management preserved (no truncation)
- Existing pipeline runs without modification
- All provider-specific tests pass

---

## Phase 3: Pipeline Automation (Days 10-13)

### Objectives
- Create automated pipeline runner
- Implement resume/skip logic
- Add progress tracking and error handling

### Deliverables

#### 3.1 Automated Runner
- [ ] Create `run_automated_evaluation.py`
- [ ] Implement `AutomatedEvaluationRunner` class
- [ ] Add command-line argument parsing
- [ ] Create dry-run mode

#### 3.2 Resume/Skip Logic
- [ ] Implement completed evaluation detection
- [ ] Add version-aware skip logic (prompts/criteria)
- [ ] Create force-rerun functionality
- [ ] Add checkpoint saving

#### 3.3 Progress Tracking
- [ ] Add progress bars with tqdm
- [ ] Implement status reporting
- [ ] Create summary statistics
- [ ] Add estimated time remaining

#### 3.4 Error Handling
- [ ] Implement retry logic with exponential backoff
- [ ] Add graceful failure handling
- [ ] Create error logging system
- [ ] Add recovery from interruptions

### Success Criteria
- Pipeline runs with single command
- Resume works correctly after interruption
- Progress clearly visible during execution
- Errors logged but don't stop pipeline

---

## Phase 4: Streamlit Dashboard (Days 14-20)

### Objectives
- Create comprehensive Streamlit dashboard
- Implement real-time monitoring
- Add analysis and visualization features

### Deliverables

#### 4.1 Dashboard Structure
- [ ] Create `dashboard/` directory
- [ ] Implement `streamlit_app.py` main file
- [ ] Create multi-page navigation
- [ ] Add sidebar configuration
- [ ] Implement data loading layer

#### 4.2 Core Pages

##### Overview Page
- [ ] Total responses and evaluations metrics
- [ ] Model performance summary cards
- [ ] Recent evaluation activity
- [ ] System health indicators

##### Model Rankings Page
- [ ] Interactive ranking table
- [ ] Radar charts for multi-criteria comparison
- [ ] Score distribution histograms
- [ ] Export rankings to CSV

##### Criteria Analysis Page
- [ ] Box plots for each criterion
- [ ] Heatmap of model vs criteria scores
- [ ] Criteria correlation analysis
- [ ] Best/worst examples per criterion

##### Provider Comparison Page
- [ ] Provider-level aggregated scores
- [ ] Cost-performance analysis (if applicable)
- [ ] Model family comparisons
- [ ] Provider reliability metrics

##### Real-Time Progress Page
- [ ] Current pipeline status
- [ ] Live progress bars
- [ ] Recent completions feed
- [ ] Error/warning alerts

##### Data Explorer Page
- [ ] Filter responses by model/sequence/run
- [ ] View individual responses
- [ ] Search functionality
- [ ] Export filtered data

#### 4.3 Visualizations
- [ ] Implement Plotly interactive charts
- [ ] Add Altair statistical plots
- [ ] Create custom visualization components
- [ ] Ensure mobile responsiveness

#### 4.4 Real-Time Features
- [ ] WebSocket or polling for live updates
- [ ] Auto-refresh capabilities
- [ ] Progress notifications
- [ ] Pipeline control (start/stop/pause)

### Success Criteria
- Dashboard loads and displays current data
- All pages functional and responsive
- Real-time updates during pipeline runs
- Export functionality works

---

## Phase 5: Cleanup & Migration (Days 21-23)

### Objectives
- Remove orphaned code
- Migrate existing functionality
- Optimize performance

### Deliverables

#### 5.1 Frontend Removal
- [ ] Remove `frontend/` directory
- [ ] Delete `Dockerfile.frontend`
- [ ] Update `docker-compose.yml`
- [ ] Remove frontend references in code
- [ ] Update documentation

#### 5.2 Code Optimization
- [ ] Remove duplicate evaluator implementations
- [ ] Consolidate configuration code
- [ ] Optimize database queries
- [ ] Add caching where appropriate

#### 5.3 Migration Scripts
- [ ] Create script to migrate existing data
- [ ] Add configuration migration tool
- [ ] Create backup procedures
- [ ] Test rollback scenarios

### Success Criteria
- No orphaned code remains
- All tests pass after cleanup
- Performance improved or maintained
- Clean git history

---

## Phase 6: Documentation & Deployment (Days 24-26)

### Objectives
- Create comprehensive documentation
- Prepare for production deployment
- Train team on new features

### Deliverables

#### 6.1 Documentation
- [ ] Update README.md for v1.5
- [ ] Create configuration guide
- [ ] Write dashboard user manual
- [ ] Add troubleshooting guide
- [ ] Create migration guide from v1.0

#### 6.2 Deployment Preparation
- [ ] Update deployment scripts
- [ ] Create v1.5 Docker images
- [ ] Test in staging environment
- [ ] Create rollback plan
- [ ] Performance benchmarking

#### 6.3 Training Materials
- [ ] Create video walkthrough
- [ ] Write quick-start guide
- [ ] Prepare FAQ document
- [ ] Schedule team training session

### Success Criteria
- Documentation complete and accurate
- Deployment process tested
- Team comfortable with new features
- Performance meets expectations

---

## Risk Mitigation

### Technical Risks
1. **LiteLLM compatibility issues**
   - Mitigation: Maintain fallback to APIEvaluator
   - Testing: Extensive provider-specific tests

2. **Context management conflicts**
   - Mitigation: Thorough testing of LangChain + LiteLLM
   - Testing: Edge cases with large contexts

3. **Dashboard performance with large datasets**
   - Mitigation: Implement pagination and caching
   - Testing: Load testing with full dataset

### Operational Risks
1. **Data loss during migration**
   - Mitigation: Comprehensive backups
   - Testing: Migration dry runs

2. **Pipeline interruption affecting production**
   - Mitigation: Feature flags for gradual rollout
   - Testing: Parallel running of old/new systems

---

## Success Metrics

### Quantitative
- [ ] All 12 models working through LiteLLM
- [ ] Pipeline execution time â‰¤ current implementation
- [ ] Dashboard page load time < 3 seconds
- [ ] 100% test coverage for new code
- [ ] Zero data loss during migration

### Qualitative
- [ ] Easier to add new models (YAML only)
- [ ] Better visibility into pipeline progress
- [ ] Improved error handling and recovery
- [ ] Enhanced analysis capabilities
- [ ] Simplified maintenance

---

## Post-Launch Plan

### Week 1 After Launch
- Monitor for issues
- Gather user feedback
- Performance optimization
- Bug fixes

### Month 1 After Launch
- Feature enhancements based on feedback
- Additional dashboard visualizations
- Extended documentation
- Team retrospective

### Future Enhancements (v1.6+)
- Multi-user support for dashboard
- Automated model discovery
- Custom evaluation criteria UI
- API endpoint for external integrations
- Advanced scheduling capabilities

---

## Sign-offs

- [ ] Technical Lead Approval
- [ ] Stakeholder Review
- [ ] Testing Complete
- [ ] Documentation Review
- [ ] Deployment Approval

---

*Last Updated: [Current Date]*
*Version: 1.0*
*Status: Planning Phase*
